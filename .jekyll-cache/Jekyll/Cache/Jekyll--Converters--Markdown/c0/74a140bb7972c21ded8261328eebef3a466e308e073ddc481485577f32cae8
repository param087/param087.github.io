I"J<p><img src="/assets/images/GSoC-logo-horizontal.svg" alt="GSoC" />
<img src="/assets/images/tf_logo_social.png" alt="TensrFlow" /></p>
<h2 id="task">Task</h2>
<p>To develop a library of traditional machine learning algorithms based on <a href="https://www.tensorflow.org/swift">Swift for TensorFlow</a>. <a href="https://www.tensorflow.org/swift">Swift for TensorFlow</a> is a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design and beyond. Its advantages include seamless integration with a modern general-purpose language, allowing for more dynamic and sophisticated models. The defined task was to create a library containing traditional machine learning algorithms.</p>

<p>In a collaborative effort with Victor Antony(GSoC student) and GSoC mentors <a href="https://twitter.com/dancherp">Dan Zheng</a>, <a href="https://twitter.com/rxwei">Richard Wei</a> and <a href="https://twitter.com/DynamicWebPaige">Paige Bailey</a>, we have created a machine learning library based on Swift for TensorFlow - <a href="https://github.com/param087/swiftML">swiftML</a>.</p>

<h2 id="community-bondingperiod">Community bonding period:</h2>
<p>I learned about Swift Package manager, <a href="https://www.tensorflow.org/swift">Swift for TensorFlow</a> project and Google’s Swift coding style. I got an insight about the Swift For TensorFlow’s Tensor, different operations on Tensor, Raw TensorFlow operations and integration with a modern general-purpose language. It helped me in writing mathematical operations and implementing overall algorithms.</p>

<h2 id="first-codingphase">First coding phase:</h2>
<p>In the first coding phase, I have implemented <code class="highlighter-rouge">linear regression</code>, <code class="highlighter-rouge">logistic regression</code>, <code class="highlighter-rouge">K-means clustering</code>, <code class="highlighter-rouge">K nearest neighbors classifier and regressor</code>. Along with this, I have implemented different helper tensor operation are <code class="highlighter-rouge">pseudoinverse of the matrix</code>, <code class="highlighter-rouge">euclidean distance</code> and <code class="highlighter-rouge">minkowski distance</code>.</p>

<ul>
  <li><code class="highlighter-rouge">Linear Regression</code> implemented using two different methods singular value decomposition and gradient descent.</li>
  <li><code class="highlighter-rouge">Logistic Regression</code> implementation is based on the sigmoid function of the dependent variable. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme.</li>
  <li><code class="highlighter-rouge">K-means clustering</code> algorithms along with <code class="highlighter-rouge">random</code> initialization of centroids supports <code class="highlighter-rouge">Kmeans++</code> - Heuristic Initialization of centroids for better performance.</li>
  <li><code class="highlighter-rouge">K nearest neighbors</code> classifier classify based on k-nearest neighbors vote, K nearest neighbors regressor predicts based on local interpolation of the targets nearest to neighbors in the training set and K nearest neighbors classifier and regressor both supports two weight functions used in the prediction are <code class="highlighter-rouge">uniform</code> - All points in each neighborhood are weighted equally and <code class="highlighter-rouge">distance</code> - weight points by the inverse of their distance.</li>
</ul>

<h2 id="second-codingphase">Second coding phase:</h2>
<p>In the second coding phase, I have implemented <code class="highlighter-rouge">principal component analysis</code>, <code class="highlighter-rouge">bernoulli naive bayes</code>, <code class="highlighter-rouge">multinomial naive bayes</code> and <code class="highlighter-rouge">gaussian naive bayes</code>. Along with this, I have implemented a helper tensor operation to compute the determinant of <code class="highlighter-rouge">singular vector decomposition (SVD)</code>.</p>
<ul>
  <li><code class="highlighter-rouge">Principal component analysis</code> implementation contains SVD based implementation along with Minka’s MLE implementation to guess the component dimension if the component count is not provided.</li>
  <li><code class="highlighter-rouge">Multinomial naive bayes</code> classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts.</li>
  <li><code class="highlighter-rouge">Bernoulli naive bayes</code> classifier is suitable for discrete data. The difference is that while Multinomial naive bayes works with occurrence counts, Bernoulli naive bayes is designed for binary features.</li>
  <li><code class="highlighter-rouge">Gaussian naive bayes</code> classifier is suitable for continuous variables.</li>
</ul>

<h2 id="third-codingphase">Third coding phase:</h2>
<p>The major part of this period was to avoid memory leaks and making design safe, documenting the code, creating end to end example notebooks.</p>

<h2 id="next-step">Next step:</h2>
<p>The <a href="https://github.com/param087/swiftML">swiftML</a> project will be continued on improvements and hoping to become a <a href="https://scikit-learn.org/">scikit-learn</a> for Swift.</p>

<h2 id="acknowledgment">Acknowledgment:</h2>
<p>I would like to acknowledge and extend my heartfelt gratitude to my mentors <a href="https://twitter.com/dancherp">Dan Zheng</a>, <a href="https://twitter.com/rxwei">Richard Wei</a> and <a href="https://twitter.com/DynamicWebPaige">Paige Bailey</a> for guiding me throughout the development process. It is great to be a part of the <a href="https://www.tensorflow.org/">TensorFlow</a> team.
I would also like to Google Summer of Code for providing me this opportunity and giving me a chance to work with TensorFlow.</p>
<ul>
  <li>Project Link: The <a href="https://github.com/param087/swiftML">swiftML</a> project repository contains all the GSoC 2019 work and current developments: <a href="https://github.com/param087/swiftML">swiftML</a></li>
  <li>End-to-end examples - <a href="https://github.com/param087/swiftML/tree/master/Notebooks">https://github.com/param087/swiftML/tree/master/Notebooks</a></li>
  <li>Following are the merged PRs - (dates differ from the actual coding period timeline as the changes were merged into above repo later than their actual implementation)
    <ul>
      <li>Linear Regression - <a href="https://github.com/param087/swiftML/pull/5">5</a></li>
      <li>Logistic Regression - <a href="https://github.com/param087/swiftML/pull/7">7</a></li>
      <li>KMeans Clustering - <a href="https://github.com/param087/swiftML/pull/9">9</a></li>
      <li>K Nearest Neighbors classifier and regressor - <a href="https://github.com/param087/swiftML/pull/11">11</a></li>
      <li>Principal Component Analysis - <a href="https://github.com/param087/swiftML/pull/15">15</a></li>
      <li>Bernoulli, Multinomial and Gaussian Naive Bayes - <a href="https://github.com/param087/swiftML/pull/13">13</a></li>
    </ul>
  </li>
</ul>
:ET